{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vehicle Safety Board of Chicago has determine that it is necessary to know and understand the primary contributory cause of a car accident. Therefore, it will be possible to recognise patterns and consequently \"it could create guidelines to implement preventive actions.\"\n",
    "Based on the data, we are going to predict if the cause of a car accident if influence by the weather, location, time or by the number of passengers.\n",
    "From the created models, we want to estimate the global/generic/general/average cost of having a injury person or the cost a average crash.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the frequency of traffic accidents between the road geometric variables, traffic characteristics, and environmental factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crashes Dataset\n",
    "crash = pd.read_csv('data/Traffic_Crashes_-_Crashes.csv')\n",
    "# crash.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# People Dataset\n",
    "people = pd.read_csv('data/Traffic_Crashes_-_People.csv')\n",
    "# people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vehicles Dataset\n",
    "vehicles = pd.read_csv('data/Traffic_Crashes_-_Vehicles.csv')\n",
    "# crashes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting specific features on crashes dataset\n",
    "crashes = crash[['CRASH_RECORD_ID', 'POSTED_SPEED_LIMIT', 'WEATHER_CONDITION', \n",
    "                 'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE', 'ROADWAY_SURFACE_COND',\n",
    "                 'ROAD_DEFECT', 'PRIM_CONTRIBUTORY_CAUSE', 'SEC_CONTRIBUTORY_CAUSE',\n",
    "                 'BEAT_OF_OCCURRENCE','CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting features on people's dataset\n",
    "peoples = people[['CRASH_RECORD_ID','PERSON_TYPE', 'SEX', 'AGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting features on vehicle's dataset\n",
    "vehicle = vehicles[['CRASH_RECORD_ID', 'VEHICLE_TYPE', 'MANEUVER']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Merging crashes with people datastes\n",
    "df_cp = pd.merge(crashes, peoples, on = 'CRASH_RECORD_ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merging df_cp with vehicles dataset\n",
    "df_x = pd.merge(df_cp, vehicle, on = 'CRASH_RECORD_ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting only class Driver\n",
    "df_t = df_x[(df_x.PERSON_TYPE == 'DRIVER') & (df_x.AGE >=16) & (df_x.AGE <= 100)]\n",
    "# df_t.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfq = df_t.drop_duplicates(subset = 'CRASH_RECORD_ID', keep='first')\n",
    "dfq.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping all nun values\n",
    "df = dfq.dropna()\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping all nun values\n",
    "df = dfq.dropna()\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic distribution of the crashes\n",
    "from sodapy import Socrata\n",
    "from shapely.geometry import shape\n",
    "\n",
    "end_point = 'data.cityofchicago.org'\n",
    "app_token = 'IaMIsAtmEfVIRb6i2lfqYvVmz'\n",
    "max_rows = 300000\n",
    "\n",
    "client = Socrata(end_point, app_token)\n",
    "chi_map = client.get(\"85ca-t3if\", limit=max_rows)\n",
    "chi_map = pd.DataFrame(chi_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Latitude and Longitude as float\n",
    "chi_map['latitude'] = chi_map['latitude'].astype(float)\n",
    "chi_map['longitude'] = chi_map['longitude'].astype(float)\n",
    "chi_map = chi_map[np.abs(chi_map['latitude'] - chi_map['latitude'].mean()) <= (10 * chi_map['latitude'].std())]\n",
    "chi_map = chi_map[np.abs(chi_map['longitude'] - chi_map['longitude'].mean()) <= (10 * chi_map['longitude'].std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "chi_map['location'] = chi_map['location'].apply(shape)\n",
    "crs = {'init': 'epsg:4326'} \n",
    "chi_map = gpd.GeoDataFrame(chi_map, geometry='location', crs=crs)\n",
    "\n",
    "chi_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chi_map.plot(markersize=0.01, color='green', figsize=(12,12))\n",
    "plt.axis('on')\n",
    "plt.title('Crashes Geographically Distributed - Chicago')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile = \"zip://data/beats_boundaries.zip\"\n",
    "beats_map = gpd.read_file(zipfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats_map.rename(columns={'beat_num': 'BEAT_OF_OCCURRENCE'}, inplace=True)\n",
    "beats_map['BEAT_OF_OCCURRENCE'] = beats_map['BEAT_OF_OCCURRENCE'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(crashes['BEAT_OF_OCCURRENCE'].value_counts())\n",
    "x['index'] = x.index\n",
    "# ignore the error\n",
    "beats_index = x.merge([x,x['index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reset_index(inplace=True)\n",
    "x.drop(columns='index', inplace=True)\n",
    "x.rename(columns={\"level_0\": \"BEAT_OF_OCCURRENCE\", \"BEAT_OF_OCCURRENCE\": \"COUNT\"}, inplace=True)\n",
    "x['BEAT_OF_OCCURRENCE'] = x['BEAT_OF_OCCURRENCE'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = beats_map.set_index('BEAT_OF_OCCURRENCE').join(x.set_index('BEAT_OF_OCCURRENCE'))\n",
    "merged.fillna(0, inplace=True)\n",
    "merged['COUNT'] = merged['COUNT'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(20, 16))\n",
    "ax = merged.plot(ax=ax, \n",
    "              column='COUNT', \n",
    "                 cmap='OrRd',\n",
    "                 alpha = .5, \n",
    "                 linewidth=.5, \n",
    "                 edgecolor='black',  \n",
    "                 legend = True,\n",
    "                 vmin=40,\n",
    "                 vmax=4000)\n",
    "ax.set_title('Concentration of car crashes in Chicago', fontsize = 20)\n",
    "ax.set_axis_off()\n",
    "fig.tight_layout\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crash['CRASH_TYPE'].value_counts().plot(kind='bar', \n",
    "                                        title =\"CRASH_TYPE\",\n",
    "                                        figsize=(10, 5),\n",
    "                                        legend=True, \n",
    "                                        fontsize=12,\n",
    "                                        color='slategrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crash['MOST_SEVERE_INJURY'].value_counts().plot(kind='bar',\n",
    "                                                title =\"MOST_SEVERE_INJURY\", \n",
    "                                                figsize=(10, 5), \n",
    "                                                legend=True,\n",
    "                                                fontsize=12,\n",
    "                                                color='slategrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "crash['DAMAGE'].value_counts().plot(kind='bar', \n",
    "                                    title =\"DAMAGE\", \n",
    "                                    figsize=(10, 5),\n",
    "                                    legend=True,\n",
    "                                    fontsize=12,\n",
    "                                    color='slategrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"CRASH_DAY_OF_WEEK\", data=crashes, color='navy')\n",
    "plt.title('Crash based on the day of the week')\n",
    "plt.xlabel('Day (1 = Sunday)', size=15)\n",
    "plt.ylabel('Crashes', size=15, rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"CRASH_HOUR\", data=crashes, color='firebrick')\n",
    "plt.title('Crash based on the hour of day')\n",
    "plt.xlabel('Hour (24H)', size=15)\n",
    "plt.ylabel('Crashes', size=15, rotation=0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash['STREET_NAME'].value_counts()[:min(20, len(crash))].plot(kind='bar',\n",
    "                                                               title =\"CRASH BY STREET\", \n",
    "                                                               figsize=(10, 5),\n",
    "                                                               legend=True,\n",
    "                                                               fontsize=12,\n",
    "                                                               color='slategrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crash['WEATHER_CONDITION'].value_counts().plot(kind='bar',\n",
    "                                               title =\"WEATHER_CONDITION\",\n",
    "                                               figsize=(10, 5),\n",
    "                                               legend=True,\n",
    "                                               fontsize=12,\n",
    "                                               color='slategrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables \n",
    "x_feats = ['WEATHER_CONDITION', 'LIGHTING_CONDITION', 'FIRST_CRASH_TYPE',\n",
    "           'ROADWAY_SURFACE_COND','ROAD_DEFECT','VEHICLE_TYPE', 'MANEUVER',\n",
    "           'SEX', 'SEC_CONTRIBUTORY_CAUSE','PERSON_TYPE']\n",
    "    \n",
    "tre = pd.get_dummies(df[x_feats], drop_first=True, dtype=float)\n",
    "tre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy = ['PRIM_CONTRIBUTORY_CAUSE']\n",
    "y_dum = pd.get_dummies(df[y_dummy], drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dum['PRIM_CONTRIBUTORY_CAUSE_FOLLOWING TOO CLOSELY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = df[['BEAT_OF_OCCURRENCE', 'CRASH_HOUR', 'CRASH_DAY_OF_WEEK']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split your X data in train and test datasets¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([qw, tre], axis=1)\n",
    "y = y_dum['PRIM_CONTRIBUTORY_CAUSE_FOLLOWING TOO CLOSELY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split X data in train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train data in train and validation datasets\n",
    "X_train_s, X_val, y_train_s, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(len(X_train_s), len(X_val), len(y_train_s), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_s:', X_train_s.shape)\n",
    "print('y_train_s:', y_train_s.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('y_val:', y_val.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the 3 datasets using StandardScaler¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_s)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_s)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42) \n",
    "tree_clf.fit(X_train_scaled,y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_p_t = tree_clf.predict_proba(X_train_scaled)[:,1]\n",
    "tr_pr_v = tree_clf.predict_proba(X_val_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC for the train dataset is\", round(roc_auc_score(y_train_s, tr_p_t), 4))\n",
    "print(\"ROC AUC for the validation dataset is\", round(roc_auc_score(y_val, tr_pr_v), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagged_tree = BaggingClassifier(DecisionTreeClassifier(criterion='gini', max_depth=5), n_estimators=20, random_state=42)\n",
    "bagged_tree.fit(X_train_scaled, y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_predi_t = bagged_tree.predict_proba(X_train_scaled)[:,1]\n",
    "bag_predi_v = bagged_tree.predict_proba(X_val_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC for the train dataset is\", round(roc_auc_score(y_train_s, bag_predi_t), 4))\n",
    "print(\"ROC AUC for the validation dataset is\", round(roc_auc_score(y_val, bag_predi_v), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "param_grid = {'n_estimators' : [20,30],\n",
    "              'bootstrap_features' : [True, False],\n",
    "              'oob_score' : [True, False],\n",
    "              }\n",
    "\n",
    "opt_model = GridSearchCV(bagged_tree,\n",
    "                         param_grid,\n",
    "                         cv=skf,\n",
    "                         scoring='accuracy',\n",
    "                         return_train_score=True)\n",
    "\n",
    "opt_model.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('The best hyperparameters are:',opt_model.best_params_)\n",
    "print(\"Best score of the model is\", round(opt_model.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = opt_model.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1, random_state=42)\n",
    "rfc.fit(X_train_scaled,y_train_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_p_t = rfc.predict_proba(X_train_scaled)[:,1]\n",
    "rfc_p_val = rfc.predict_proba(X_val_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC for the train dataset is\", round(roc_auc_score(y_train_s, rfc_p_t), 4))\n",
    "print(\"ROC AUC for the validation dataset is\", round(roc_auc_score(y_val, rfc_p_val), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfc_param_grid = {'max_depth':range(1,11,50),\n",
    "                  'min_samples_leaf':[5,10,15]}\n",
    "\n",
    "opt_rfc = GridSearchCV(rfc, \n",
    "                       rfc_param_grid,\n",
    "                       cv=skf, \n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "opt_rfc.fit(X_train_scaled, y_train_s)\n",
    "\n",
    "print('The best hyperparameters are:',opt_rfc.best_params_)\n",
    "print(\"Best score of the model is\", round(opt_rfc.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rfc.fit(X_train_scaled, y_train_s)\n",
    "print('Values of the optimised hyperparameters\\nfor the best model found:',\n",
    "      opt_rfc.best_params_)\n",
    "round(opt_rfc.best_score_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = opt_rfc.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_t,tpr_t,thr_t = roc_curve(y_train_s,bag_predi_t)\n",
    "fpr_v,tpr_v,thr_v = roc_curve(y_val,bag_predi_v)\n",
    "\n",
    "plt.title('ROC Validation')\n",
    "plt.plot(fpr_t, tpr_t, 'red', label = 'AUC')\n",
    "plt.plot(fpr_v, tpr_v, 'b', label = 'AUC val')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFP = 650\n",
    "TNC = 0\n",
    "FNC = 47921\n",
    "TPC = 20728\n",
    "prevalence = .025\n",
    "m = ((1 - prevalence) / prevalence ) * ((CFP - TNC) / (FNC - TPC))\n",
    "\n",
    "print( f' m coefficient is {round(m,2)}')\n",
    "\n",
    "# FP cost is the mean of the ambulance dispatch with and without health insurance\n",
    "# https://health.costhelper.com/ambulance.html\n",
    "\n",
    "# FN costs are a mean of all possible car injuries levels.\n",
    "# Fatalities are excluded given the almost absence from our target crash type.\n",
    "# extracted from the KABCO table, page 15\n",
    "# https://safety.fhwa.dot.gov/hsip/docs/fhwasa17071.pdf\n",
    "\n",
    "# TP costs is the sum of the means of medical bills and crash repairs.\n",
    "# extracted from a Verisk Analysis report from 2013, prices adjusted for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_train_s,bag_predi_t)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding TPR and FPR for threshold 0.4\n",
    "thresh_40 = threshold[65]\n",
    "fpr_40 = fpr[65]\n",
    "tpr_40 = tpr[65]\n",
    "\n",
    "fm_40 = tpr_40 - m*fpr_40\n",
    "\n",
    "print(f' The fm for the .4 threshold is {round(fm_40,2)}')\n",
    "\n",
    "# finding TPR and FPR for threshold 0.5\n",
    "thresh_50 = threshold[46]\n",
    "fpr_50 = fpr[46]\n",
    "tpr_50 = tpr[46]\n",
    "\n",
    "fm_50 = tpr_50 - m*fpr_50\n",
    "\n",
    "print(f' The fm for the .5 threshold is {round(fm_50,2)}')\n",
    "\n",
    "# finding TPR and FPR for threshold 0.6\n",
    "thresh_60 = threshold[33]\n",
    "fpr_60 = fpr[33]\n",
    "tpr_60 = tpr[33]\n",
    "\n",
    "fm_60 = tpr_60 - m*fpr_60\n",
    "\n",
    "print(f' The fm for the .6 threshold is {round(fm_60,2)}')\n",
    "\n",
    "# finding TPR and FPR for threshold 0.7\n",
    "thresh_70 = threshold[25]\n",
    "fpr_70 = fpr[25]\n",
    "tpr_70 = tpr[25]\n",
    "\n",
    "fm_70 = tpr_70 - m*fpr_70\n",
    "\n",
    "print(f' The fm for the .7 threshold is {round(fm_70,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating the confusion matrix with sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_s, bag_predi_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
